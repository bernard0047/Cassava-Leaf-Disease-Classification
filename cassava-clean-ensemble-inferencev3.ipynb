{"cells":[{"metadata":{"_cell_guid":"44e0e8dd-3f69-4b09-bbf9-4b61ab211837","_uuid":"6ea60225-2ed8-4464-94ea-426eede4f725","execution":{"iopub.execute_input":"2021-01-26T12:29:21.030518Z","iopub.status.busy":"2021-01-26T12:29:21.029695Z","iopub.status.idle":"2021-01-26T12:29:21.032123Z","shell.execute_reply":"2021-01-26T12:29:21.032765Z"},"papermill":{"duration":0.027472,"end_time":"2021-01-26T12:29:21.032888","exception":false,"start_time":"2021-01-26T12:29:21.005416","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"package_path = '../input/pytorch-image-models/pytorch-image-models-master' #'../input/efficientnet-pytorch-07/efficientnet_pytorch-0.7.0'\nimport sys; sys.path.append(package_path)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from albumentations import (\n    HorizontalFlip, VerticalFlip, IAAPerspective, ShiftScaleRotate, CLAHE, RandomRotate90, Transpose, ShiftScaleRotate,\n    Blur, OpticalDistortion, GridDistortion, HueSaturationValue, IAAAdditiveGaussianNoise, GaussNoise, MotionBlur,\n    MedianBlur, IAAPiecewiseAffine, RandomResizedCrop, IAASharpen, IAAEmboss, RandomBrightnessContrast, Flip,\n    OneOf, Compose, Normalize, Cutout, CoarseDropout, ShiftScaleRotate, CenterCrop, Resize\n)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"5ec73de8-81f5-4331-8a73-4c6ab37e5923","_uuid":"3607d2c4-66c6-4e40-8432-9640aefbc6ed","execution":{"iopub.execute_input":"2021-01-26T12:29:21.084172Z","iopub.status.busy":"2021-01-26T12:29:21.083476Z","iopub.status.idle":"2021-01-26T12:29:24.49087Z","shell.execute_reply":"2021-01-26T12:29:24.489729Z"},"papermill":{"duration":3.43735,"end_time":"2021-01-26T12:29:24.491006","exception":false,"start_time":"2021-01-26T12:29:21.053656","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"from torch.utils.data.sampler import SequentialSampler, RandomSampler\nfrom sklearn.model_selection import GroupKFold, StratifiedKFold\nfrom sklearn.metrics import roc_auc_score, log_loss\nfrom  torch.cuda.amp import autocast, GradScaler\nfrom torch.utils.data import Dataset,DataLoader\nfrom albumentations.pytorch import ToTensorV2\nfrom scipy.ndimage.interpolation import zoom\nfrom contextlib import contextmanager\nfrom torchvision import transforms\nimport matplotlib.pyplot as plt\nfrom datetime import datetime\nfrom sklearn import metrics\nfrom skimage import io\nfrom tqdm import tqdm\nfrom glob import glob\nfrom torch import nn\nimport pandas as pd\nimport torchvision\nimport numpy as np\nimport warnings\nimport pydicom\nimport sklearn\nimport joblib\nimport random\nimport torch\nimport timm\nimport time\nimport cv2\nimport os","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Efficientnet_B3 (Without SnapMix):"},{"metadata":{"_cell_guid":"2dfd253a-3417-4db4-90e3-0e81f9416617","_uuid":"7c0aa19c-8715-4c3c-8959-d8c1f5d06700","execution":{"iopub.execute_input":"2021-01-26T12:29:24.539908Z","iopub.status.busy":"2021-01-26T12:29:24.537983Z","iopub.status.idle":"2021-01-26T12:29:24.540644Z","shell.execute_reply":"2021-01-26T12:29:24.541109Z"},"papermill":{"duration":0.029912,"end_time":"2021-01-26T12:29:24.541226","exception":false,"start_time":"2021-01-26T12:29:24.511314","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"CFG = {\n    'fold_num': 10,\n    'seed': 719,\n    'model_arch': 'tf_efficientnet_b3_ns',\n    'img_size': 512,\n    'epochs': 10,\n    'train_bs': 16,\n    'valid_bs': 16,\n    'lr': 1e-4,\n    'num_workers': 4,\n    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n    'verbose_step': 1,\n    'device': 'cuda:0',\n    'tta': 3,\n    'used_epochs': [6,7,8,9],\n    'weights': [1,1,1,1]\n}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"61aa057d-14f4-4b45-b934-79c00bd77c95","_uuid":"87a6b7b3-e2cd-420a-9f2e-bd1feb6ad934","execution":{"iopub.execute_input":"2021-01-26T12:29:24.588367Z","iopub.status.busy":"2021-01-26T12:29:24.587752Z","iopub.status.idle":"2021-01-26T12:29:24.632883Z","shell.execute_reply":"2021-01-26T12:29:24.631873Z"},"papermill":{"duration":0.071336,"end_time":"2021-01-26T12:29:24.632995","exception":false,"start_time":"2021-01-26T12:29:24.561659","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\nsubmission = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"46bd2623-1b6a-48c1-8788-87e28ce52802","_uuid":"ede84a90-2726-4515-9215-e357861b7b19","execution":{"iopub.execute_input":"2021-01-26T12:29:24.887393Z","iopub.status.busy":"2021-01-26T12:29:24.886712Z","iopub.status.idle":"2021-01-26T12:29:25.173193Z","shell.execute_reply":"2021-01-26T12:29:25.173683Z"},"papermill":{"duration":0.316153,"end_time":"2021-01-26T12:29:25.173815","exception":false,"start_time":"2021-01-26T12:29:24.857662","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \ndef get_img(path):\n    im_bgr = cv2.imread(path)\n    im_rgb = im_bgr[:, :, ::-1]\n    #print(im_rgb)\n    return im_rgb","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"d0971419-2664-4fa7-b07b-56313828a953","_uuid":"4ece1b4d-4050-4053-9ea6-b857d2fa420f","execution":{"iopub.execute_input":"2021-01-26T12:29:25.290431Z","iopub.status.busy":"2021-01-26T12:29:25.289526Z","iopub.status.idle":"2021-01-26T12:29:25.297779Z","shell.execute_reply":"2021-01-26T12:29:25.298595Z"},"papermill":{"duration":0.045939,"end_time":"2021-01-26T12:29:25.298769","exception":false,"start_time":"2021-01-26T12:29:25.25283","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class CassavaDataset(Dataset):\n    def __init__(\n        self, df, data_root, transforms=None, output_label=True\n    ):\n        \n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.transforms = transforms\n        self.data_root = data_root\n        self.output_label = output_label\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        \n        # get labels\n        if self.output_label:\n            target = self.df.iloc[index]['label']\n          \n        path = \"{}/{}\".format(self.data_root, self.df.iloc[index]['image_id'])\n        \n        img  = get_img(path)\n        \n        if self.transforms:\n            img = self.transforms(image=img)['image']\n            \n        # do label smoothing\n        if self.output_label == True:\n            return img, target\n        else:\n            return img","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"881eddac-48cd-4e94-9137-edf7b39a1f08","_uuid":"de12a378-4de0-4fa1-8e68-82461b2b3920","execution":{"iopub.execute_input":"2021-01-26T12:29:25.478531Z","iopub.status.busy":"2021-01-26T12:29:25.477541Z","iopub.status.idle":"2021-01-26T12:29:26.285846Z","shell.execute_reply":"2021-01-26T12:29:26.287046Z"},"papermill":{"duration":0.867591,"end_time":"2021-01-26T12:29:26.287221","exception":false,"start_time":"2021-01-26T12:29:25.41963","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def get_train_transforms():\n    return Compose([\n            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            CoarseDropout(p=0.5),\n            Cutout(p=0.5),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n  \n        \ndef get_valid_transforms():\n    return Compose([\n            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n            Resize(CFG['img_size'], CFG['img_size']),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n\ndef get_inference_transforms():\n    return Compose([\n            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"6ddbd2ec-d85d-45bd-ab12-5397f3bd0e6d","_uuid":"7f2b7c09-92f9-49b5-b400-7af3fe89bd60","execution":{"iopub.execute_input":"2021-01-26T12:29:26.469426Z","iopub.status.busy":"2021-01-26T12:29:26.468357Z","iopub.status.idle":"2021-01-26T12:29:26.473304Z","shell.execute_reply":"2021-01-26T12:29:26.475879Z"},"papermill":{"duration":0.069649,"end_time":"2021-01-26T12:29:26.476078","exception":false,"start_time":"2021-01-26T12:29:26.406429","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class CassvaImgClassifier(nn.Module):\n    def __init__(self, model_arch, n_class, pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_arch, pretrained=pretrained)\n        n_features = self.model.classifier.in_features\n        self.model.classifier = nn.Linear(n_features, n_class)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"51830ef0-50d2-4d14-8123-e2717d18b59f","_uuid":"e40787da-b43f-4e15-bf16-a968bc661c25","execution":{"iopub.execute_input":"2021-01-26T12:29:26.601691Z","iopub.status.busy":"2021-01-26T12:29:26.599862Z","iopub.status.idle":"2021-01-26T12:29:26.602442Z","shell.execute_reply":"2021-01-26T12:29:26.602922Z"},"papermill":{"duration":0.038266,"end_time":"2021-01-26T12:29:26.603036","exception":false,"start_time":"2021-01-26T12:29:26.56477","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def inference_one_epoch(model, data_loader, device):\n    model.eval()\n\n    image_preds_all = []\n    \n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs) in pbar:\n        imgs = imgs.to(device).float()\n        \n        image_preds = model(imgs)   #output = model(input)\n        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n        \n    \n    image_preds_all = np.concatenate(image_preds_all, axis=0)\n    return image_preds_all","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"c610e286-a881-49dd-8ac6-fa6ff86fd1e9","_uuid":"c006ba82-6d0e-40d5-b5e8-290aa36414f3","execution":{"iopub.execute_input":"2021-01-26T12:29:26.682989Z","iopub.status.busy":"2021-01-26T12:29:26.682119Z","iopub.status.idle":"2021-01-26T12:29:40.301202Z","shell.execute_reply":"2021-01-26T12:29:40.302034Z"},"papermill":{"duration":13.669203,"end_time":"2021-01-26T12:29:40.302238","exception":false,"start_time":"2021-01-26T12:29:26.633035","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"if __name__ == '__main__':\n     # for training only, need nightly build pytorch\n\n    seed_everything(CFG['seed'])\n    \n    folds = StratifiedKFold(n_splits=CFG['fold_num']).split(np.arange(train.shape[0]), train.label.values)\n    \n    for fold, (trn_idx, val_idx) in enumerate(folds):\n        # we'll train fold 0 first\n        if fold > 0:\n            break \n\n        print('Inference fold {} started'.format(fold))\n\n        valid_ = train.loc[val_idx,:].reset_index(drop=True)\n        valid_ds = CassavaDataset(valid_, '../input/cassava-leaf-disease-classification/train_images/', transforms=get_inference_transforms(), output_label=False)\n        \n        test = pd.DataFrame()\n        test['image_id'] = list(os.listdir('../input/cassava-leaf-disease-classification/test_images/'))\n        test_ds = CassavaDataset(test, '../input/cassava-leaf-disease-classification/test_images/', transforms=get_inference_transforms(), output_label=False)\n        \n        val_loader = torch.utils.data.DataLoader(\n            valid_ds, \n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            shuffle=False,\n            pin_memory=False,\n        )\n        \n        tst_loader = torch.utils.data.DataLoader(\n            test_ds, \n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            shuffle=False,\n            pin_memory=False,\n        )\n\n        device = torch.device(CFG['device'])\n        model = CassvaImgClassifier(CFG['model_arch'], train.label.nunique()).to(device)\n        \n        val_preds = []\n        eff_preds = []\n        \n        #for epoch in range(CFG['epochs']-3):\n        for i, epoch in enumerate(CFG['used_epochs']):    \n            model.load_state_dict(torch.load('../input/cassava-eff-results-1/eff_b3_without_snap_32_5_10/{}_fold_{}_{}'.format(CFG['model_arch'], fold, epoch)))\n            \n            with torch.no_grad():\n                for _ in range(CFG['tta']):\n                    #val_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, val_loader, device)]\n                    eff_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, tst_loader, device)]\n\n        #val_preds = np.mean(val_preds, axis=0) \n        eff_preds = np.sum(eff_preds, axis=0) \n        \n        #print('fold {} validation loss = {:.5f}'.format(fold, log_loss(valid_.label.values, val_preds)))\n        #print('fold {} validation accuracy = {:.5f}'.format(fold, (valid_.label.values==np.argmax(val_preds, axis=1)).mean()))\n        \n        del model\n        torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"eff_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ViT Base_Patch16_384:\n"},{"metadata":{"_cell_guid":"243a7d06-5afe-4c66-be9c-603ccf5ec3c0","_uuid":"2c5bff1e-8ea8-4d78-8876-64c38be180f4","execution":{"iopub.execute_input":"2021-01-26T12:29:40.568025Z","iopub.status.busy":"2021-01-26T12:29:40.567272Z","iopub.status.idle":"2021-01-26T12:29:40.56971Z","shell.execute_reply":"2021-01-26T12:29:40.570281Z"},"papermill":{"duration":0.047406,"end_time":"2021-01-26T12:29:40.570409","exception":false,"start_time":"2021-01-26T12:29:40.523003","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"package_path = '../input/pytorch-image-models/pytorch-image-models-master' #'../input/efficientnet-pytorch-07/efficientnet_pytorch-0.7.0'\nimport sys; sys.path.append(package_path)","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"7353e1f6-dd73-48db-a43b-7af43594a037","_uuid":"9bb7df35-ee67-4933-a0e5-1da42a6c8feb","execution":{"iopub.execute_input":"2021-01-26T12:29:40.668732Z","iopub.status.busy":"2021-01-26T12:29:40.666644Z","iopub.status.idle":"2021-01-26T12:29:40.669498Z","shell.execute_reply":"2021-01-26T12:29:40.670044Z"},"papermill":{"duration":0.059083,"end_time":"2021-01-26T12:29:40.670182","exception":false,"start_time":"2021-01-26T12:29:40.611099","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"CFG = {\n    'fold_num': 5,\n    'seed': 719,\n    'model_arch': 'vit_base_patch16_384',\n    'img_size': 384,\n    'epochs': 10,\n    'train_bs': 32,\n    'valid_bs': 32,\n    'lr': 1e-4,\n    'num_workers': 4,\n    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n    'verbose_step': 1,\n    'device': 'cuda:0',\n    'tta': 3,\n    'used_epochs': [5,6,7,8,9],\n    'weights': [1,1,1,1,1]\n}","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"54fe56fb-985e-431b-9d01-a6282daad0a2","_uuid":"d4628a00-6904-4d5f-8cfb-ea7c052064b5","execution":{"iopub.execute_input":"2021-01-26T12:29:40.770131Z","iopub.status.busy":"2021-01-26T12:29:40.769542Z","iopub.status.idle":"2021-01-26T12:29:40.775878Z","shell.execute_reply":"2021-01-26T12:29:40.775127Z"},"papermill":{"duration":0.060272,"end_time":"2021-01-26T12:29:40.775985","exception":false,"start_time":"2021-01-26T12:29:40.715713","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"submission = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\nsubmission.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"bcb2d7fb-75f1-4fa2-bb17-d5599b575cfe","_uuid":"e82212ef-e558-474f-ba89-fcfdd107e4f3","execution":{"iopub.execute_input":"2021-01-26T12:29:40.868033Z","iopub.status.busy":"2021-01-26T12:29:40.867168Z","iopub.status.idle":"2021-01-26T12:29:40.871372Z","shell.execute_reply":"2021-01-26T12:29:40.870854Z"},"papermill":{"duration":0.052768,"end_time":"2021-01-26T12:29:40.871473","exception":false,"start_time":"2021-01-26T12:29:40.818705","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"class CassvaImgClassifier(nn.Module):\n    def __init__(self, model_arch, n_class, pretrained=False):\n        super().__init__()\n        MODEL_PATH = \"../input/vit-base-models-pretrained-pytorch/jx_vit_base_p16_384-83fb41ba.pth\"\n        self.model = timm.create_model(\"vit_base_patch16_384\", pretrained=False)\n        \n        self.model.load_state_dict(torch.load(MODEL_PATH))\n\n        self.model.head = nn.Linear(self.model.head.in_features, 5)\n        \n    def forward(self, x):\n        x = self.model(x)\n        return x","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"72bfa458-c33a-4031-b184-b48e3bbef24b","_uuid":"6e7f3af3-ae45-4794-8114-abc860bc4672","execution":{"iopub.execute_input":"2021-01-26T12:29:40.962747Z","iopub.status.busy":"2021-01-26T12:29:40.962094Z","iopub.status.idle":"2021-01-26T12:29:40.966007Z","shell.execute_reply":"2021-01-26T12:29:40.965447Z"},"papermill":{"duration":0.053087,"end_time":"2021-01-26T12:29:40.966105","exception":false,"start_time":"2021-01-26T12:29:40.913018","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"def inference_one_epoch(model, data_loader, device):\n    model.eval()\n\n    image_preds_all = []\n    \n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs) in pbar:\n        imgs = imgs.to(device).float()\n        \n        image_preds = model(imgs)   #output = model(input)\n        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n        \n    \n    image_preds_all = np.concatenate(image_preds_all, axis=0)\n    return image_preds_all","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"8026c939-1ad8-40ff-8316-1dec66a2b739","_uuid":"2ac914e6-8bcf-4998-9fec-e9d1a1613601","execution":{"iopub.execute_input":"2021-01-26T12:29:41.069611Z","iopub.status.busy":"2021-01-26T12:29:41.068723Z","iopub.status.idle":"2021-01-26T12:30:05.676078Z","shell.execute_reply":"2021-01-26T12:30:05.675309Z"},"papermill":{"duration":24.66718,"end_time":"2021-01-26T12:30:05.676226","exception":false,"start_time":"2021-01-26T12:29:41.009046","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"if __name__ == '__main__':\n     # for training only, need nightly build pytorch\n\n    seed_everything(CFG['seed'])\n    \n    folds = StratifiedKFold(n_splits=CFG['fold_num']).split(np.arange(train.shape[0]), train.label.values)\n    \n    for fold, (trn_idx, val_idx) in enumerate(folds):\n        # we'll train fold 0 first\n        if fold > 0:\n            break \n\n        print('Inference fold {} started'.format(fold))\n\n        valid_ = train.loc[val_idx,:].reset_index(drop=True)\n        valid_ds = CassavaDataset(valid_, '../input/cassava-leaf-disease-classification/train_images/', transforms=get_inference_transforms(), output_label=False)\n        \n        test = pd.DataFrame()\n        test['image_id'] = list(os.listdir('../input/cassava-leaf-disease-classification/test_images/'))\n        test_ds = CassavaDataset(test, '../input/cassava-leaf-disease-classification/test_images/', transforms=get_inference_transforms(), output_label=False)\n        \n        val_loader = torch.utils.data.DataLoader(\n            valid_ds, \n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            shuffle=False,\n            pin_memory=False,\n        )\n        \n        tst_loader = torch.utils.data.DataLoader(\n            test_ds, \n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            shuffle=False,\n            pin_memory=False,\n        )\n\n        device = torch.device(CFG['device'])\n        model = CassvaImgClassifier(CFG['model_arch'], train.label.nunique()).to(device)\n        \n        val_preds = []\n        vit_preds = []\n        \n        #for epoch in range(CFG['epochs']-3):\n        for i, epoch in enumerate(CFG['used_epochs']):    \n            model.load_state_dict(torch.load('../input/cassava-eff-results-1/vision_trans_64_4_10/{}_fold_{}_{}'.format(CFG['model_arch'], fold, epoch)))\n            \n            with torch.no_grad():\n                for _ in range(CFG['tta']):\n                    #val_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, val_loader, device)]\n                    vit_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, tst_loader, device)]\n\n        #val_preds = np.mean(val_preds, axis=0) \n        vit_preds = np.sum(vit_preds, axis=0) \n        \n        #print('fold {} validation loss = {:.5f}'.format(fold, log_loss(valid_.label.values, val_preds)))\n        #print('fold {} validation accuracy = {:.5f}'.format(fold, (valid_.label.values==np.argmax(val_preds, axis=1)).mean()))\n        \n        del model\n        torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"vit_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ResNeXt50_32x4d:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR = './'\nMODEL_DIR = '../input/cassava-eff-results-1/resnext_snap_inc_32_5_10/'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n\nTRAIN_PATH = '../input/cassava-leaf-disease-classification/train_images'\nTEST_PATH = '../input/cassava-leaf-disease-classification/test_images'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    debug=False\n    num_workers=4\n    model_name='resnext50_32x4d'\n    size=512\n    batch_size=16\n    seed=42\n    target_size=5\n    target_col='label'\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n    train=False\n    inference=True\n    tta=3\n    used_epochs= [8,9,10,11,12]\n    weights= [1,1,1,1,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# Utils\n# ====================================================\ndef get_score(y_true, y_pred):\n    return accuracy_score(y_true, y_pred)\n\n\n@contextmanager\ndef timer(name):\n    t0 = time.time()\n    LOGGER.info(f'[{name}] start')\n    yield\n    LOGGER.info(f'[{name}] done in {time.time() - t0:.0f} s.')\n\n\ndef init_logger(log_file=OUTPUT_DIR+'inference.log'):\n    from logging import getLogger, INFO, FileHandler,  Formatter,  StreamHandler\n    logger = getLogger(__name__)\n    logger.setLevel(INFO)\n    handler1 = StreamHandler()\n    handler1.setFormatter(Formatter(\"%(message)s\"))\n    handler2 = FileHandler(filename=log_file)\n    handler2.setFormatter(Formatter(\"%(message)s\"))\n    logger.addHandler(handler1)\n    logger.addHandler(handler2)\n    return logger\n\n#LOGGER = init_logger()\n\n\ndef seed_torch(seed=42):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n\nseed_torch(seed=CFG.seed)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\ntest = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# # ====================================================\n# # Dataset\n# # ====================================================\nclass CassavaDataset(Dataset):\n    def __init__(\n        self, df, data_root, transforms=None, output_label=True\n    ):\n        \n        super().__init__()\n        self.df = df.reset_index(drop=True).copy()\n        self.transforms = transforms\n        self.data_root = data_root\n        self.output_label = output_label\n    \n    def __len__(self):\n        return self.df.shape[0]\n    \n    def __getitem__(self, index: int):\n        \n        # get labels\n        if self.output_label:\n            target = self.df.iloc[index]['label']\n          \n        path = \"{}/{}\".format(self.data_root, self.df.iloc[index]['image_id'])\n        \n        img  = get_img(path)\n        \n        if self.transforms:\n            img = self.transforms(image=img)['image']\n            \n        # do label smoothing\n        if self.output_label == True:\n            return img, target\n        else:\n            return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# Transforms\n# ====================================================\ndef get_transforms(*, data):\n    \n    if data == 'train':\n        return Compose([\n            #Resize(CFG.size, CFG.size),\n            RandomResizedCrop(CFG.size, CFG.size),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\n\n    elif data == 'valid':\n        return Compose([\n            Resize(CFG.size, CFG.size),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\ndef get_inference_transforms():\n    return Compose([\n            RandomResizedCrop(CFG.size, CFG.size),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomResNext(nn.Module):\n    def __init__(self, model_name='resnext50_32x4d', pretrained=False):\n        super().__init__()\n        backbone = timm.create_model(model_name, pretrained=pretrained)\n        n_features = backbone.fc.in_features\n        self.model = nn.Sequential(*backbone.children())[:-2]\n        self.classifier = nn.Linear(n_features, CFG.target_size)\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        \n    def forward_features(self, x):\n        x = self.model(x)\n        return x\n\n    def forward(self, x):\n        feats = self.forward_features(x)\n        x = self.pool(feats).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x, feats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# Helper functions\n# ====================================================\ndef seed_everything(seed):\n    random.seed(seed)\n    os.environ['PYTHONHASHSEED'] = str(seed)\n    np.random.seed(seed)\n    torch.manual_seed(seed)\n    torch.cuda.manual_seed(seed)\n    torch.backends.cudnn.deterministic = True\n    torch.backends.cudnn.benchmark = True\n    \ndef inference(model, states, test_loader, device):\n    model.to(device)\n    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n    probs = []\n    for i, (images) in tk0:\n        images = images.to(device)\n        avg_preds = []\n        for state in states:\n            model.load_state_dict(state['model'])\n            model.eval()\n            with torch.no_grad():\n                #y_preds = model(images)\n                y_preds,_ = model(images) #snapmix\n            avg_preds.append(y_preds.softmax(1).to('cpu').numpy())\n        avg_preds = np.mean(avg_preds, axis=0)\n        probs.append(avg_preds)\n    probs = np.concatenate(probs)\n    return probs\ndef get_img(path):\n    im_bgr = cv2.imread(path)\n    im_rgb = im_bgr[:, :, ::-1]\n    #print(im_rgb)\n    return im_rgb","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference_one_epoch(model, data_loader, device):\n    model.eval()\n\n    image_preds_all = []\n    \n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs) in pbar:\n        imgs = imgs.to(device).float()\n        \n        #image_preds = model(imgs)\n        image_preds, _ = model(imgs)   #for snapmix inference\n        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n        \n    \n    image_preds_all = np.concatenate(image_preds_all, axis=0)\n    return image_preds_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == '__main__':\n     # for training only, need nightly build pytorch\n\n    seed_everything(CFG.seed)\n    \n    folds = StratifiedKFold(n_splits=CFG.n_fold).split(np.arange(train.shape[0]), train.label.values)\n    \n    for fold, (trn_idx, val_idx) in enumerate(folds):\n        # we'll train fold 0 first\n        if fold > 0:\n            break \n\n        print('Inference fold {} started'.format(fold))\n\n        valid_ = train.loc[val_idx,:].reset_index(drop=True)\n        valid_ds = CassavaDataset(valid_, '../input/cassava-leaf-disease-classification/train_images/', transforms=get_inference_transforms(), output_label=False)\n        \n        test = pd.DataFrame()\n        test['image_id'] = list(os.listdir('../input/cassava-leaf-disease-classification/test_images/'))\n        test_ds = CassavaDataset(test, '../input/cassava-leaf-disease-classification/test_images/', transforms=get_inference_transforms(), output_label=False)\n        \n        \n        val_loader = torch.utils.data.DataLoader(\n            valid_ds, \n            batch_size=CFG.batch_size,\n            num_workers=CFG.num_workers,\n            shuffle=False,\n            pin_memory=False,\n        )\n        \n        tst_loader = torch.utils.data.DataLoader(\n            test_ds, \n            batch_size=CFG.batch_size,\n            num_workers=CFG.num_workers,\n            shuffle=False,\n            pin_memory=False,\n        )\n\n        device = torch.device('cuda:0')\n        model = CustomResNext(CFG.model_name, pretrained=False).to(device)\n        \n        val_preds = []\n        resv1tta = []\n        \n        #for epoch in range(CFG['epochs']-3):\n        for i, epoch in enumerate(CFG.used_epochs):    \n            model.load_state_dict(torch.load(MODEL_DIR+f'{CFG.model_name}_fold{i}_best.pth')['model'])\n            \n            with torch.no_grad():\n                for _ in range(CFG.tta):\n                    #val_preds += [CFG.weights[i]/sum(CFG.weights)/CFG.tta*inference_one_epoch(model, val_loader, device)]\n                    resv1tta += [CFG.weights[i]/sum(CFG.weights)/CFG.tta*inference_one_epoch(model, tst_loader, device)]\n\n        #val_preds = np.mean(val_preds, axis=0) \n        resv1tta = np.sum(resv1tta, axis=0) \n        \n        #print('fold {} validation loss = {:.5f}'.format(fold, log_loss(valid_.label.values, val_preds)))\n        #print('fold {} validation accuracy = {:.5f}'.format(fold, (valid_.label.values==np.argmax(val_preds, axis=1)).mean()))\n        \n        del model\n        torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# Dataset\n# ====================================================\nclass TestDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.df = df\n        self.file_names = df['image_id'].values\n        self.transform = transform\n        \n    def __len__(self):\n        return len(self.df)\n\n    def __getitem__(self, idx):\n        file_name = self.file_names[idx]\n        file_path = f'{TEST_PATH}/{file_name}'\n        image = cv2.imread(file_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        if self.transform:\n            augmented = self.transform(image=image)\n            image = augmented['image']\n        return image\n    \n# ====================================================\n# Helper functions\n# ====================================================\ndef inference(model, states, test_loader, device):\n    model.to(device)\n    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n    probs = []\n    for i, (images) in tk0:\n        images = images.to(device)\n        avg_preds = []\n        for state in states:\n            model.load_state_dict(state['model'])\n            model.eval()\n            with torch.no_grad():\n                #y_preds = model(images)\n                y_preds, _ = model(images) #for snapmix\n            avg_preds.append(y_preds.softmax(1).to('cpu').numpy())\n        avg_preds = np.mean(avg_preds, axis=0)\n        probs.append(avg_preds)\n    probs = np.concatenate(probs)\n    return probs\n\n# ====================================================\n# inference\n# ====================================================\nmodel = CustomResNext(CFG.model_name, pretrained=False)\nstates = [torch.load(MODEL_DIR+f'{CFG.model_name}_fold{fold}_best.pth') for fold in range(5)]\ntest_dataset = TestDataset(test, transform=get_transforms(data='valid'))\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, \n                         num_workers=CFG.num_workers, pin_memory=True)\nresv1notta = inference(model, states, test_loader, device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_DIR = '../input/cassava-resnext50-results-1/resnext50_master_512_32_5_10/'\n# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    debug=False\n    num_workers=4\n    model_name='resnext50_32x4d'\n    size=512\n    batch_size=16\n    seed=42\n    target_size=5\n    target_col='label'\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n    train=False\n    inference=True\n    tta=3\n    #used_epochs= [0,1,2,3,4]\n    used_folds = [0,2,3,4]\n    weights= [1,1,1,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# inference\n# ====================================================\nmodel = CustomResNext(CFG.model_name, pretrained=False)\nstates = [torch.load(MODEL_DIR+f'{CFG.model_name}_fold{fold}_best.pth') for fold in CFG.used_folds]\ntest_dataset = TestDataset(test, transform=get_transforms(data='valid'))\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, \n                         num_workers=CFG.num_workers, pin_memory=True)\nresv2notta = inference(model, states, test_loader, device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"res_preds = 0.5*resv1tta + 0.25*resv1notta + 0.25*resv2notta\nres_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# EfficientNet_B3 (With SnapMix):"},{"metadata":{"trusted":true},"cell_type":"code","source":"CFG = {\n    'fold_num': 10,\n    'seed': 719,\n    'model_arch': 'tf_efficientnet_b3_ns',\n    'img_size': 512,\n    'epochs': 25,\n    'train_bs': 32,\n    'valid_bs': 32,\n    'lr': 1e-4,\n    'num_workers': 4,\n    'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n    'verbose_step': 1,\n    'device': 'cuda:0',\n    'tta': 3,\n    'used_epochs': [6,7,8,9],\n    'weights': [1,1,1,1]\n}","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CassvaImgClassifier(nn.Module):\n    def __init__(self, model_arch, n_class, pretrained=False):\n        super().__init__()\n        backbone = timm.create_model(CFG['model_arch'], pretrained=pretrained)\n        n_features = backbone.classifier.in_features  #backbone.classifier.in_features\n        self.model = nn.Sequential(*backbone.children())[:-2]\n        self.classifier = nn.Linear(n_features, n_class)\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n\n    def forward_features(self, x):\n        x = self.model(x)\n        return x\n\n    def forward(self, x):\n        feats = self.forward_features(x)\n        x = self.pool(feats).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x, feats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def get_train_transforms():\n    return Compose([\n            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            CoarseDropout(p=0.5),\n            Cutout(p=0.5),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n  \n        \ndef get_valid_transforms():\n    return Compose([\n            CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n            Resize(CFG['img_size'], CFG['img_size']),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)\n\ndef get_inference_transforms():\n    return Compose([\n            RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference_one_epoch(model, data_loader, device):\n    model.eval()\n\n    image_preds_all = []\n    \n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs) in pbar:\n        imgs = imgs.to(device).float()\n        \n        image_preds,_ = model(imgs)   #output = model(input)\n        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n        \n    \n    image_preds_all = np.concatenate(image_preds_all, axis=0)\n    return image_preds_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == '__main__':\n     # for training only, need nightly build pytorch\n\n    seed_everything(CFG['seed'])\n    \n    folds = StratifiedKFold(n_splits=CFG['fold_num']).split(np.arange(train.shape[0]), train.label.values)\n    \n    for fold, (trn_idx, val_idx) in enumerate(folds):\n        # we'll train fold 0 first\n        if fold > 0:\n            break \n\n        print('Inference fold {} started'.format(fold))\n\n        valid_ = train.loc[val_idx,:].reset_index(drop=True)\n        valid_ds = CassavaDataset(valid_, '../input/cassava-leaf-disease-classification/train_images/', transforms=get_inference_transforms(), output_label=False)\n        \n        test = pd.DataFrame()\n        test['image_id'] = list(os.listdir('../input/cassava-leaf-disease-classification/test_images/'))\n        test_ds = CassavaDataset(test, '../input/cassava-leaf-disease-classification/test_images/', transforms=get_inference_transforms(), output_label=False)\n        \n        val_loader = torch.utils.data.DataLoader(\n            valid_ds, \n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            shuffle=False,\n            pin_memory=False,\n        )\n        \n        tst_loader = torch.utils.data.DataLoader(\n            test_ds, \n            batch_size=CFG['valid_bs'],\n            num_workers=CFG['num_workers'],\n            shuffle=False,\n            pin_memory=False,\n        )\n\n        device = torch.device(CFG['device'])\n        model = CassvaImgClassifier(CFG['model_arch'], train.label.nunique()).to(device)\n        \n        val_preds = []\n        effsnapmix_preds = []\n        \n        #for epoch in range(CFG['epochs']-3):\n        for i, epoch in enumerate(CFG['used_epochs']):    \n            model.load_state_dict(torch.load('../input/cassava-eff-results-2/eff_b3_smapmix_512_32_5_10/{}_fold_{}_{}'.format(CFG['model_arch'], fold, epoch)))\n            \n            with torch.no_grad():\n                for _ in range(CFG['tta']):\n                    #val_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, val_loader, device)]\n                    effsnapmix_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, tst_loader, device)]\n\n        #val_preds = np.mean(val_preds, axis=0) \n        effsnapmix_preds = np.sum(effsnapmix_preds, axis=0) \n        \n        #print('fold {} validation loss = {:.5f}'.format(fold, log_loss(valid_.label.values, val_preds)))\n        #print('fold {} validation accuracy = {:.5f}'.format(fold, (valid_.label.values==np.argmax(val_preds, axis=1)).mean()))\n        \n        del model\n        torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"effsnapmix_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# ResNeSt_50d:"},{"metadata":{"trusted":true},"cell_type":"code","source":"# CFG = {\n#     'fold_num': 5,\n#     'seed': 719,\n#     'model_arch': 'resnest50d',\n#     'img_size': 512,\n#     'epochs': 10,\n#     'train_bs': 32,\n#     'valid_bs': 32,\n#     'lr': 1e-4,\n#     'num_workers': 4,\n#     'accum_iter': 1, # suppoprt to do batch accumulation for backprop with effectively larger batch size\n#     'verbose_step': 1,\n#     'device': 'cuda:0',\n#     'tta': 5,\n#     'used_epochs': [6,7,8,9],\n#     'weights': [1,1,1,1]\n# }","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\n# submission = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# class CassavaDataset(Dataset):\n#     def __init__(\n#         self, df, data_root, transforms=None, output_label=True\n#     ):\n        \n#         super().__init__()\n#         self.df = df.reset_index(drop=True).copy()\n#         self.transforms = transforms\n#         self.data_root = data_root\n#         self.output_label = output_label\n    \n#     def __len__(self):\n#         return self.df.shape[0]\n    \n#     def __getitem__(self, index: int):\n        \n#         # get labels\n#         if self.output_label:\n#             target = self.df.iloc[index]['label']\n          \n#         path = \"{}/{}\".format(self.data_root, self.df.iloc[index]['image_id'])\n        \n#         img  = get_img(path)\n        \n#         if self.transforms:\n#             img = self.transforms(image=img)['image']\n            \n#         # do label smoothing\n#         if self.output_label == True:\n#             return img, target\n#         else:\n#             return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# def get_train_transforms():\n#     return Compose([\n#             RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n#             Transpose(p=0.5),\n#             HorizontalFlip(p=0.5),\n#             VerticalFlip(p=0.5),\n#             ShiftScaleRotate(p=0.5),\n#             HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n#             RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n#             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n#             CoarseDropout(p=0.5),\n#             Cutout(p=0.5),\n#             ToTensorV2(p=1.0),\n#         ], p=1.)\n  \n        \n# def get_valid_transforms():\n#     return Compose([\n#             CenterCrop(CFG['img_size'], CFG['img_size'], p=1.),\n#             Resize(CFG['img_size'], CFG['img_size']),\n#             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n#             ToTensorV2(p=1.0),\n#         ], p=1.)\n\n# def get_inference_transforms():\n#     return Compose([\n#             RandomResizedCrop(CFG['img_size'], CFG['img_size']),\n#             Transpose(p=0.5),\n#             HorizontalFlip(p=0.5),\n#             VerticalFlip(p=0.5),\n#             HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n#             RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n#             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n#             ToTensorV2(p=1.0),\n#         ], p=1.)\n\n# # %% [markdown]\n# # # Model\n\n# # %% [code]\n# class CassvaImgClassifier(nn.Module):\n#     def __init__(self, model_arch, n_class, pretrained=False):\n#         super().__init__()\n#         backbone = timm.create_model('resnest50d', pretrained=pretrained)\n#         n_features = backbone.fc.in_features  #backbone.classifier.in_features\n#         self.model = nn.Sequential(*backbone.children())[:-2]\n#         self.classifier = nn.Linear(n_features, n_class)\n#         self.pool = nn.AdaptiveAvgPool2d((1, 1))\n\n#     def forward_features(self, x):\n#         x = self.model(x)\n#         return x\n\n#     def forward(self, x):\n#         feats = self.forward_features(x)\n#         x = self.pool(feats).view(x.size(0), -1)\n#         x = self.classifier(x)\n#         return x, feats\n\n# # %% [markdown]\n# # # Main Loop\n\n# # %% [code]\n# def inference_one_epoch(model, data_loader, device):\n#     model.eval()\n\n#     image_preds_all = []\n    \n#     pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n#     for step, (imgs) in pbar:\n#         imgs = imgs.to(device).float()\n        \n#         image_preds,_ = model(imgs)   #output = model(input)\n#         image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n        \n    \n#     image_preds_all = np.concatenate(image_preds_all, axis=0)\n#     return image_preds_all\n\n# # %% [code]\n# if __name__ == '__main__':\n#      # for training only, need nightly build pytorch\n\n#     seed_everything(CFG['seed'])\n    \n#     folds = StratifiedKFold(n_splits=CFG['fold_num']).split(np.arange(train.shape[0]), train.label.values)\n    \n#     for fold, (trn_idx, val_idx) in enumerate(folds):\n#         # we'll train fold 0 first\n#         if fold == 1: \n\n#             print('Inference fold {} started'.format(fold))\n\n#             valid_ = train.loc[val_idx,:].reset_index(drop=True)\n#             valid_ds = CassavaDataset(valid_, '../input/cassava-leaf-disease-classification/train_images/', transforms=get_inference_transforms(), output_label=False)\n\n#             test = pd.DataFrame()\n#             test['image_id'] = list(os.listdir('../input/cassava-leaf-disease-classification/test_images/'))\n#             test_ds = CassavaDataset(test, '../input/cassava-leaf-disease-classification/test_images/', transforms=get_inference_transforms(), output_label=False)\n\n#             val_loader = torch.utils.data.DataLoader(\n#                 valid_ds, \n#                 batch_size=CFG['valid_bs'],\n#                 num_workers=CFG['num_workers'],\n#                 shuffle=False,\n#                 pin_memory=False,\n#             )\n\n#             tst_loader = torch.utils.data.DataLoader(\n#                 test_ds, \n#                 batch_size=CFG['valid_bs'],\n#                 num_workers=CFG['num_workers'],\n#                 shuffle=False,\n#                 pin_memory=False,\n#             )\n\n#             device = torch.device(CFG['device'])\n#             model = CassvaImgClassifier(CFG['model_arch'], train.label.nunique()).to(device)\n\n#             #val_preds = []\n#             resnest_preds = []\n\n#             #for epoch in range(CFG['epochs']-3):\n#             for i, epoch in enumerate(CFG['used_epochs']):    \n#                 model.load_state_dict(torch.load('../input/resnest-firstfold-898/{}_fold_{}_{}'.format(CFG['model_arch'], fold, epoch)))\n\n#                 with torch.no_grad():\n#                     for _ in range(CFG['tta']):\n#                         #val_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, val_loader, device)]\n#                         resnest_preds += [CFG['weights'][i]/sum(CFG['weights'])/CFG['tta']*inference_one_epoch(model, tst_loader, device)]\n\n#             #val_preds = np.mean(val_preds, axis=0) \n# #             resnest_preds = np.sum(resnest_preds, axis=0) \n\n#             #print('fold {} validation loss = {:.5f}'.format(fold, log_loss(valid_.label.values, val_preds)))\n#             #print('fold {} validation accuracy = {:.5f}'.format(fold, (valid_.label.values==np.argmax(val_preds, axis=1)).mean()))\n\n#             del model\n#             torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# resnest_preds","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# SEResNeXt:\n1. SEResNeXt101"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# Directory settings\n# ====================================================\nimport os\n\nOUTPUT_DIR = './'\nMODEL_DIR = '../input/cassava-seresnext-results-1/seresnext101/'\nif not os.path.exists(OUTPUT_DIR):\n    os.makedirs(OUTPUT_DIR)\n\nTRAIN_PATH = '../input/cassavapreprocessed/train_images/train_images'\nTEST_PATH = '../input/cassava-leaf-disease-classification/test_images'","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    debug=False\n    num_workers=4\n    model_name='seresnext101_32x4d'\n    size=512\n    batch_size=16\n    seed=42\n    target_size=5\n    target_col='label'\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n    train=False\n    inference=True\n    tta=3\n    used_folds= [0,1,2,3,4]\n    weights= [1,1,1,1,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train = pd.read_csv('../input/cassava-leaf-disease-classification/train.csv')\ntest = pd.read_csv('../input/cassava-leaf-disease-classification/sample_submission.csv')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from __future__ import print_function, division, absolute_import\nfrom collections import OrderedDict\nimport math\n\nimport torch.nn as nn\nfrom torch.utils import model_zoo\n\npretrained_settings = {\n    'se_resnext101_32x4d': {\n        'imagenet': {\n            'url': 'http://data.lip6.fr/cadene/pretrainedmodels/se_resnext101_32x4d-3b2fe3d8.pth',\n            'input_space': 'RGB',\n            'input_size': [3, 224, 224],\n            'input_range': [0, 1],\n            'mean': [0.485, 0.456, 0.406],\n            'std': [0.229, 0.224, 0.225],\n            'num_classes': 1000\n        }\n    },\n}\nclass SEModule(nn.Module):\n\n    def __init__(self, channels, reduction):\n        super(SEModule, self).__init__()\n        self.avg_pool = nn.AdaptiveAvgPool2d(1)\n        self.fc1 = nn.Conv2d(channels, channels // reduction, kernel_size=1,\n                             padding=0)\n        self.relu = nn.ReLU(inplace=True)\n        self.fc2 = nn.Conv2d(channels // reduction, channels, kernel_size=1,\n                             padding=0)\n        self.sigmoid = nn.Sigmoid()\n\n    def forward(self, x):\n        module_input = x\n        x = self.avg_pool(x)\n        x = self.fc1(x)\n        x = self.relu(x)\n        x = self.fc2(x)\n        x = self.sigmoid(x)\n        return module_input * x\n\n\nclass Bottleneck(nn.Module):\n    \"\"\"\n    Base class for bottlenecks that implements `forward()` method.\n    \"\"\"\n    def forward(self, x):\n        residual = x\n\n        out = self.conv1(x)\n        out = self.bn1(out)\n        out = self.relu(out)\n\n        out = self.conv2(out)\n        out = self.bn2(out)\n        out = self.relu(out)\n\n        out = self.conv3(out)\n        out = self.bn3(out)\n\n        if self.downsample is not None:\n            residual = self.downsample(x)\n\n        out = self.se_module(out) + residual\n        out = self.relu(out)\n\n        return out\n\n\nclass SEBottleneck(Bottleneck):\n    \"\"\"\n    Bottleneck for SENet154.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes * 2, kernel_size=1, bias=False)\n        self.bn1 = nn.BatchNorm2d(planes * 2)\n        self.conv2 = nn.Conv2d(planes * 2, planes * 4, kernel_size=3,\n                               stride=stride, padding=1, groups=groups,\n                               bias=False)\n        self.bn2 = nn.BatchNorm2d(planes * 4)\n        self.conv3 = nn.Conv2d(planes * 4, planes * 4, kernel_size=1,\n                               bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNetBottleneck(Bottleneck):\n    \"\"\"\n    ResNet bottleneck with a Squeeze-and-Excitation module. It follows Caffe\n    implementation and uses `stride=stride` in `conv1` and not in `conv2`\n    (the latter is used in the torchvision implementation of ResNet).\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None):\n        super(SEResNetBottleneck, self).__init__()\n        self.conv1 = nn.Conv2d(inplanes, planes, kernel_size=1, bias=False,\n                               stride=stride)\n        self.bn1 = nn.BatchNorm2d(planes)\n        self.conv2 = nn.Conv2d(planes, planes, kernel_size=3, padding=1,\n                               groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(planes)\n        self.conv3 = nn.Conv2d(planes, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SEResNeXtBottleneck(Bottleneck):\n    \"\"\"\n    ResNeXt bottleneck type C with a Squeeze-and-Excitation module.\n    \"\"\"\n    expansion = 4\n\n    def __init__(self, inplanes, planes, groups, reduction, stride=1,\n                 downsample=None, base_width=4):\n        super(SEResNeXtBottleneck, self).__init__()\n        width = math.floor(planes * (base_width / 64)) * groups\n        self.conv1 = nn.Conv2d(inplanes, width, kernel_size=1, bias=False,\n                               stride=1)\n        self.bn1 = nn.BatchNorm2d(width)\n        self.conv2 = nn.Conv2d(width, width, kernel_size=3, stride=stride,\n                               padding=1, groups=groups, bias=False)\n        self.bn2 = nn.BatchNorm2d(width)\n        self.conv3 = nn.Conv2d(width, planes * 4, kernel_size=1, bias=False)\n        self.bn3 = nn.BatchNorm2d(planes * 4)\n        self.relu = nn.ReLU(inplace=True)\n        self.se_module = SEModule(planes * 4, reduction=reduction)\n        self.downsample = downsample\n        self.stride = stride\n\n\nclass SENet(nn.Module):\n\n    def __init__(self, block, layers, groups, reduction, dropout_p=0.2,\n                 inplanes=128, input_3x3=True, downsample_kernel_size=3,\n                 downsample_padding=1, num_classes=1000):\n        super(SENet, self).__init__()\n        self.inplanes = inplanes\n        if input_3x3:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, 64, 3, stride=2, padding=1,\n                                    bias=False)),\n                ('bn1', nn.BatchNorm2d(64)),\n                ('relu1', nn.ReLU(inplace=True)),\n                ('conv2', nn.Conv2d(64, 64, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn2', nn.BatchNorm2d(64)),\n                ('relu2', nn.ReLU(inplace=True)),\n                ('conv3', nn.Conv2d(64, inplanes, 3, stride=1, padding=1,\n                                    bias=False)),\n                ('bn3', nn.BatchNorm2d(inplanes)),\n                ('relu3', nn.ReLU(inplace=True)),\n            ]\n        else:\n            layer0_modules = [\n                ('conv1', nn.Conv2d(3, inplanes, kernel_size=7, stride=2,\n                                    padding=3, bias=False)),\n                ('bn1', nn.BatchNorm2d(inplanes)),\n                ('relu1', nn.ReLU(inplace=True)),\n            ]\n        # To preserve compatibility with Caffe weights `ceil_mode=True`\n        # is used instead of `padding=1`.\n        layer0_modules.append(('pool', nn.MaxPool2d(3, stride=2,\n                                                    ceil_mode=True)))\n        self.layer0 = nn.Sequential(OrderedDict(layer0_modules))\n        self.layer1 = self._make_layer(\n            block,\n            planes=64,\n            blocks=layers[0],\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=1,\n            downsample_padding=0\n        )\n        self.layer2 = self._make_layer(\n            block,\n            planes=128,\n            blocks=layers[1],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer3 = self._make_layer(\n            block,\n            planes=256,\n            blocks=layers[2],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.layer4 = self._make_layer(\n            block,\n            planes=512,\n            blocks=layers[3],\n            stride=2,\n            groups=groups,\n            reduction=reduction,\n            downsample_kernel_size=downsample_kernel_size,\n            downsample_padding=downsample_padding\n        )\n        self.avg_pool = nn.AvgPool2d(7, stride=1)\n        self.dropout = nn.Dropout(dropout_p) if dropout_p is not None else None\n        self.last_linear = nn.Linear(512 * block.expansion, num_classes)\n\n    def _make_layer(self, block, planes, blocks, groups, reduction, stride=1,\n                    downsample_kernel_size=1, downsample_padding=0):\n        downsample = None\n        if stride != 1 or self.inplanes != planes * block.expansion:\n            downsample = nn.Sequential(\n                nn.Conv2d(self.inplanes, planes * block.expansion,\n                          kernel_size=downsample_kernel_size, stride=stride,\n                          padding=downsample_padding, bias=False),\n                nn.BatchNorm2d(planes * block.expansion),\n            )\n\n        layers = []\n        layers.append(block(self.inplanes, planes, groups, reduction, stride,\n                            downsample))\n        self.inplanes = planes * block.expansion\n        for i in range(1, blocks):\n            layers.append(block(self.inplanes, planes, groups, reduction))\n\n        return nn.Sequential(*layers)\n\n    def features(self, x):\n        x = self.layer0(x)\n        x = self.layer1(x)\n        x = self.layer2(x)\n        x = self.layer3(x)\n        x = self.layer4(x)\n        return x\n\n    def logits(self, x):\n        x = self.avg_pool(x)\n        if self.dropout is not None:\n            x = self.dropout(x)\n        x = x.view(x.size(0), -1)\n        x = self.last_linear(x)\n        return x\n\n    def forward(self, x):\n        x = self.features(x)\n        x = self.logits(x)\n        return x\n\n\ndef initialize_pretrained_model(model, num_classes, settings):\n    assert num_classes == settings['num_classes'], \\\n        'num_classes should be {}, but is {}'.format(\n            settings['num_classes'], num_classes)\n    model.load_state_dict(model_zoo.load_url(settings['url']))\n    model.input_space = settings['input_space']\n    model.input_size = settings['input_size']\n    model.input_range = settings['input_range']\n    model.mean = settings['mean']\n    model.std = settings['std']\n\ndef se_resnext101(num_classes=1000, pretrained='imagenet'):\n    model = SENet(SEResNeXtBottleneck, [3, 4, 23, 3], groups=32, reduction=16,\n                  dropout_p=None, inplanes=64, input_3x3=False,\n                  downsample_kernel_size=1, downsample_padding=0,\n                  num_classes=num_classes)\n    if pretrained is not None:\n        settings = pretrained_settings['se_resnext101_32x4d'][pretrained]\n        initialize_pretrained_model(model, num_classes, settings)\n    return model","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# Transforms\n# ====================================================\ndef get_transforms(*, data):\n    \n    if data == 'train':\n        return Compose([\n            #Resize(CFG.size, CFG.size),\n            RandomResizedCrop(CFG.size, CFG.size),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            ShiftScaleRotate(p=0.5),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\n\n    elif data == 'valid':\n        return Compose([\n            Resize(CFG.size, CFG.size),\n            Normalize(\n                mean=[0.485, 0.456, 0.406],\n                std=[0.229, 0.224, 0.225],\n            ),\n            ToTensorV2(),\n        ])\ndef get_inference_transforms():\n    return Compose([\n            RandomResizedCrop(CFG.size, CFG.size),\n            Transpose(p=0.5),\n            HorizontalFlip(p=0.5),\n            VerticalFlip(p=0.5),\n            HueSaturationValue(hue_shift_limit=0.2, sat_shift_limit=0.2, val_shift_limit=0.2, p=0.5),\n            RandomBrightnessContrast(brightness_limit=(-0.1,0.1), contrast_limit=(-0.1, 0.1), p=0.5),\n            Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225], max_pixel_value=255.0, p=1.0),\n            ToTensorV2(p=1.0),\n        ], p=1.)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomSEResNext101(nn.Module):\n    def __init__(self,pretrained='imagenet'):\n        super().__init__()\n        backbone = se_resnext101(pretrained=pretrained)\n        n_features = backbone.last_linear.in_features\n        self.model = nn.Sequential(*backbone.children())[:-2]\n        self.classifier = nn.Linear(n_features, CFG.target_size)\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        \n    def forward_features(self, x):\n        x = self.model(x)\n        return x\n\n    def forward(self, x):\n        feats = self.forward_features(x)\n        x = self.pool(feats).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x, feats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def inference_one_epoch(model, data_loader, device):\n    model.eval()\n\n    image_preds_all = []\n    \n    pbar = tqdm(enumerate(data_loader), total=len(data_loader))\n    for step, (imgs) in pbar:\n        imgs = imgs.to(device).float()\n        \n        #image_preds = model(imgs)\n        image_preds, _ = model(imgs)   #for snapmix inference\n        image_preds_all += [torch.softmax(image_preds, 1).detach().cpu().numpy()]\n        \n    \n    image_preds_all = np.concatenate(image_preds_all, axis=0)\n    return image_preds_all","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"if __name__ == '__main__':\n     # for training only, need nightly build pytorch\n\n    seed_everything(CFG.seed)\n    \n    folds = StratifiedKFold(n_splits=CFG.n_fold).split(np.arange(train.shape[0]), train.label.values)\n    \n    for fold, (trn_idx, val_idx) in enumerate(folds):\n        # we'll train fold 0 first\n        if fold > 0:\n            break \n\n        print('Inference fold {} started'.format(fold))\n\n        valid_ = train.loc[val_idx,:].reset_index(drop=True)\n        valid_ds = CassavaDataset(valid_, '../input/cassava-leaf-disease-classification/train_images/', transforms=get_inference_transforms(), output_label=False)\n        \n        test = pd.DataFrame()\n        test['image_id'] = list(os.listdir('../input/cassava-leaf-disease-classification/test_images/'))\n        test_ds = CassavaDataset(test, '../input/cassava-leaf-disease-classification/test_images/', transforms=get_inference_transforms(), output_label=False)\n        \n        \n        val_loader = torch.utils.data.DataLoader(\n            valid_ds, \n            batch_size=CFG.batch_size,\n            num_workers=CFG.num_workers,\n            shuffle=False,\n            pin_memory=False,\n        )\n        \n        tst_loader = torch.utils.data.DataLoader(\n            test_ds, \n            batch_size=CFG.batch_size,\n            num_workers=CFG.num_workers,\n            shuffle=False,\n            pin_memory=False,\n        )\n\n        device = torch.device('cuda:0')\n        model = CustomSEResNext101(pretrained=None).to(device)\n        \n        val_preds = []\n        serespreds101 = []\n        \n        #for epoch in range(CFG['epochs']-3):\n        for i, fold in enumerate(CFG.used_folds):    \n            model.load_state_dict(torch.load(MODEL_DIR+f'{CFG.model_name}_fold{fold}_best.pth')['model'])\n            \n            with torch.no_grad():\n                for _ in range(CFG.tta):\n                    #val_preds += [CFG.weights[i]/sum(CFG.weights)/CFG.tta*inference_one_epoch(model, val_loader, device)]\n                    serespreds101 += [CFG.weights[i]/sum(CFG.weights)/CFG.tta*inference_one_epoch(model, tst_loader, device)]\n\n        #val_preds = np.mean(val_preds, axis=0) \n        serespreds101 = np.sum(serespreds101, axis=0) \n        \n        #print('fold {} validation loss = {:.5f}'.format(fold, log_loss(valid_.label.values, val_preds)))\n        #print('fold {} validation accuracy = {:.5f}'.format(fold, (valid_.label.values==np.argmax(val_preds, axis=1)).mean()))\n        \n        del model\n        torch.cuda.empty_cache()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"2. SEResNeXt50"},{"metadata":{"trusted":true},"cell_type":"code","source":"# ====================================================\n# CFG\n# ====================================================\nclass CFG:\n    debug=False\n    num_workers=4\n    model_name='seresnext50_32x4d'\n    size=512\n    batch_size=16\n    seed=42\n    target_size=5\n    target_col='label'\n    n_fold=5\n    trn_fold=[0, 1, 2, 3, 4]\n    train=False\n    inference=True\n    tta=3\n    used_folds= [0,1,2,3,4]\n    weights= [1,1,1,1,1]","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"class CustomSEResNext50(nn.Module):\n    def __init__(self, model_name=CFG.model_name, pretrained=False):\n        super().__init__()\n        backbone = timm.create_model(model_name, pretrained=pretrained)\n        n_features = backbone.fc.in_features\n        self.model = nn.Sequential(*backbone.children())[:-2]\n        self.classifier = nn.Linear(n_features, CFG.target_size)\n        self.pool = nn.AdaptiveAvgPool2d((1, 1))\n        \n    def forward_features(self, x):\n        x = self.model(x)\n        return x\n\n    def forward(self, x):\n        feats = self.forward_features(x)\n        x = self.pool(feats).view(x.size(0), -1)\n        x = self.classifier(x)\n        return x, feats","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"MODEL_DIR = '../input/cassava-seresnext-results-1/seresnext50_with_snapmix/'\n\ndef inference(model, states, test_loader, device):\n    model.to(device)\n    tk0 = tqdm(enumerate(test_loader), total=len(test_loader))\n    probs = []\n    for i, (images) in tk0:\n        images = images.to(device)\n        avg_preds = []\n        for state in states:\n            model.load_state_dict(state['model'])\n            model.eval()\n            with torch.no_grad():\n                #y_preds = model(images)\n                y_preds, _ = model(images) #for snapmix\n            avg_preds.append(y_preds.softmax(1).to('cpu').numpy())\n        avg_preds = np.mean(avg_preds, axis=0)\n        probs.append(avg_preds)\n    probs = np.concatenate(probs)\n    return probs\n\nmodel = CustomSEResNext50(CFG.model_name, pretrained=False)\nstates = [torch.load(MODEL_DIR+f'{CFG.model_name}_fold{fold}_best.pth') for fold in range(5)]\ntest_dataset = TestDataset(test, transform=get_transforms(data='valid'))\ntest_loader = DataLoader(test_dataset, batch_size=CFG.batch_size, shuffle=False, \n                         num_workers=CFG.num_workers, pin_memory=True)\nserespreds50 = inference(model, states, test_loader, device)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"seresblend = 0.7*serespreds50 + 0.3*serespreds101","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Ensemble All:"},{"metadata":{"_cell_guid":"179eeb01-f6ac-47c3-b592-cecf40e13e1c","_uuid":"3d0ed064-bdce-4008-b361-92a7ddabcd15","execution":{"iopub.execute_input":"2021-01-26T12:30:05.800734Z","iopub.status.busy":"2021-01-26T12:30:05.799924Z","iopub.status.idle":"2021-01-26T12:30:05.803362Z","shell.execute_reply":"2021-01-26T12:30:05.803855Z"},"papermill":{"duration":0.069183,"end_time":"2021-01-26T12:30:05.803983","exception":false,"start_time":"2021-01-26T12:30:05.7348","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"PREDS = 0.22*eff_preds + 0.23*vit_preds + 0.12*res_preds + 0.18*effsnapmix_preds + 0.25*seresblend","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"np.sum(PREDS)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# for i in PREDS:\n#     i[-1] = 0.975*i[-1]","execution_count":null,"outputs":[]},{"metadata":{"execution":{"iopub.execute_input":"2021-01-26T12:30:05.921267Z","iopub.status.busy":"2021-01-26T12:30:05.9205Z","iopub.status.idle":"2021-01-26T12:30:05.924792Z","shell.execute_reply":"2021-01-26T12:30:05.924221Z"},"papermill":{"duration":0.065437,"end_time":"2021-01-26T12:30:05.924892","exception":false,"start_time":"2021-01-26T12:30:05.859455","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"print(eff_preds,vit_preds,res_preds,effsnapmix_preds,seresblend,'',PREDS,sep='\\n')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test['label'] = np.argmax(PREDS, axis=1)\ntest.head()","execution_count":null,"outputs":[]},{"metadata":{"_cell_guid":"3e306fb1-b2f2-428e-894a-5412f4df304f","_uuid":"665f1e8a-114c-47b5-8633-739600dd4258","execution":{"iopub.execute_input":"2021-01-26T12:30:06.041769Z","iopub.status.busy":"2021-01-26T12:30:06.041107Z","iopub.status.idle":"2021-01-26T12:30:06.286082Z","shell.execute_reply":"2021-01-26T12:30:06.284904Z"},"papermill":{"duration":0.305567,"end_time":"2021-01-26T12:30:06.286216","exception":false,"start_time":"2021-01-26T12:30:05.980649","status":"completed"},"tags":[],"trusted":true},"cell_type":"code","source":"test.to_csv('submission.csv', index=False)","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}